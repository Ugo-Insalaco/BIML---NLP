{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnUpEJRs8Ia2"
      },
      "source": [
        "# MOD 4.6 - TD 3 :  **Sequence-to-sequence with RNN and Transformers**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbNq7bWVtVIP"
      },
      "source": [
        "## 1. Projet description\n",
        "\n",
        "\n",
        "The project aims at exploring and comparing a traditional RNN with a Transformer model for a translation task. We will translate the parallel corpus [Tatoeba](https://tatoeba.org/fr) from English to French."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfXjpb_z8KGe"
      },
      "source": [
        "### 1.1 Sequence-to-Sequence introduction\n",
        "- Typical sequence-to-sequence (seq2seq) models are encoder-decoder models, which usually consists of two parts, the encoder and decoder, respectively. These two parts can be implemented with recurrent neural network (RNN) or **transformer**, primarily to deal with input/output sequences of dynamic length.\n",
        "- **Encoder** encodes a sequence of inputs, such as text, video or audio, into a single vector, which can be viewed as the abstractive representation of the inputs, containing information of the whole sequence.\n",
        "- **Decoder** decodes the vector output of encoder one step at a time, until the final output sequence is complete. Every decoding step is affected by previous step(s). Generally, one would add \"< BOS >\" at the begining of the sequence to indicate start of decoding, and \"< EOS >\" at the end to indicate end of decoding.\n",
        "\n",
        "![seq2seq](https://i.imgur.com/0zeDyuI.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkgNFDTQ8gTd"
      },
      "source": [
        "### 1.2 Work Description\n",
        "- English to French (traditional) Translation\n",
        "  - Input: an English sentence         \n",
        "  - Output: the French translation \n",
        "\n",
        "- **TODO**\n",
        "    - Train a simple RNN seq2seq to achieve translation\n",
        "    - Switch to transformer model to boost the performance\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFh9yfMt8rBo"
      },
      "source": [
        "## 2. Working environment\n",
        "\n",
        "### 2.1 Google colab\n",
        "\n",
        "This project will be done by using a Jupyter notebook on the Google Colab platform. To import the notebook on Google Colab, follow these steps :\n",
        "1. Open the following link : https://colab.research.google.com/ \n",
        "\n",
        "2. If a window opens itself : just go on the Import tab\n",
        "\n",
        "3. Else : Click-on File > Import a notebook\n",
        "\n",
        "4. Import the TD3-seq2seq.ipynb file\n",
        "\n",
        "You need to change the type of execution :\n",
        "\n",
        "Click on : “Runtime” → “Change runtime type” → “Hardware accelerator”. \n",
        "\n",
        "Then select \"GPU\".\n",
        "\n",
        "\n",
        "**WARNING** : The free version of Google Collab offers limited GPU usage. So think twice before running your code or you will be forced to use CPUs, which will make your code run much slower.\n",
        "\n",
        "### 2.2 Set up the environment \n",
        "\n",
        "Download and import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnkB8B-09SqM"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics\n",
        "! python -m spacy download fr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmgsY7jY8DVq"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torchmetrics import BLEUScore\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "from torch import Tensor\n",
        "import io\n",
        "import time\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn import (TransformerEncoder, TransformerDecoder,\n",
        "                      TransformerEncoderLayer, TransformerDecoderLayer)\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJJOUz3e9C6m"
      },
      "source": [
        "\n",
        "Fix random seed and start a CUDA session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w3Vt4ac8Si2"
      },
      "outputs": [],
      "source": [
        "seed = 73\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  \n",
        "np.random.seed(seed)  \n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt0Ipk4k9Skn"
      },
      "source": [
        "## 3. Dataset pre-processing for RNN \n",
        "\n",
        "### 3.1 Overview\n",
        "\n",
        "The data set is composed of 135842 sentences in English and French. To ease the training we will select only the sentences with a maximum length of 15. The sen- tences are relatively simple with a English vocabulary size of 3281 and a French vocabulary size of 4971. Examples :\n",
        "\n",
        "— I’m not old. → Je ne suis pas vieux. \n",
        "\n",
        "— You’re funny. → Vous etes marrant.\n",
        "\n",
        "\n",
        "\n",
        "### 3.2 Download and extract data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EipFwWjaJMa"
      },
      "outputs": [],
      "source": [
        "!wget  https://download.pytorch.org/tutorial/data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY7X1T2Gac4j"
      },
      "outputs": [],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zztrvR6z9nOq"
      },
      "outputs": [],
      "source": [
        "src_lang = 'fr'\n",
        "tgt_lang = 'en'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6eBQBZI_UHu"
      },
      "source": [
        "### 3.3 Sentence preprocessing\n",
        "#### 3.3.1 Indexing words\n",
        "\n",
        "We will need a unique index per word to use as the inputs and targets of the networks later. To keep track of all this we will use a helper class called Lang which has word → index (word2index) and index → word (index2word) dictionaries, as well as a count of each word word2count which will be used to replace rare words later.\n",
        "\n",
        "![preprocessing](https://pytorch.org/tutorials/_images/word-encoding.png)\n",
        "\n",
        "The Lang class allows to create an index dictionary for each word given a set of vocabulary for a given language.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIcjRmsJ-VGz"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmPweEWs_5f7"
      },
      "source": [
        "#### 3.3.2 From Unicode to ASCII and text normalization\n",
        "\n",
        "The files are all in Unicode. Before creating the index dictionary, the input text must be formatted :\n",
        "\n",
        "— Be converted to Ascii, see the function `unicodeToAscii`\n",
        "\n",
        "— Be normalized, by being in lowercase, trimmed, and without non-letter characters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmZF4hhmABuE"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znmi29xUAWYY"
      },
      "source": [
        "### Read langs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S313At0ka_-R"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6gu5J4zduN9"
      },
      "source": [
        "#### 3.3.3  Sentences filtering\n",
        "\n",
        "We will then filter the sentences to eliminate all examples with a length greater than 15, and keep those starting with predefined words see the variable `eng_prefixes`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3tKp_OeFPFG"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 15\n",
        "MIN_LENGTH = 1\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[0].split(' ')) > MIN_LENGTH and \\\n",
        "        len(p[1].split(' ')) > MIN_LENGTH and \\\n",
        "        len(p[0].split('.')) == 2 and \\\n",
        "        len(p[1].split('.')) == 2 and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-Iiie6zrgaG"
      },
      "source": [
        "#### 3.3.4  Run the pre-processing\n",
        "\n",
        "The full process for preparing the data is:\n",
        "\n",
        "+ Read text file and split into lines, split lines into pairs\n",
        "\n",
        "+ Normalize text, filter by length and content\n",
        "\n",
        "+ Make word lists from sentences in pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkVDglckL9RZ"
      },
      "outputs": [],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs( lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "# NOte: reverse == True\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4yYDhsp35IL"
      },
      "source": [
        "### 3.4 Split the data set\n",
        "\n",
        "**TODO** In the next cell, write a function to randomly assign 90% of the pairs to a training set and 10% of them to an evaluation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poUFKvih6W7G"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "# Warning: the pairs for the training are in a variable named pairs_train\n",
        "# and for the the evaluation in a variable named pairs_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ey_EvMc1-H83"
      },
      "outputs": [],
      "source": [
        "print(\"Len train : \", len(pairs_train), \"Len test : \", len(pairs_test) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbtjmieFa3d_"
      },
      "source": [
        "## 4.  RNN Model\n",
        "\n",
        "### 4.1 Overview \n",
        "\n",
        "A Recurrent Neural Network, or RNN, is a network that operates on a sequence and uses its own output as input for subsequent steps.\n",
        "\n",
        "A Sequence to Sequence network, or seq2seq network, or Encoder Decoder network, is a model consisting of two RNNs called the encoder and decoder. The encoder reads an input sequence and outputs a single vector, and the decoder reads that vector to produce an output sequence.\n",
        "\n",
        "![RNN_fig1](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
        "\n",
        "\n",
        "To improve upon this model we will use an attention mechanism, which lets the decoder learn to focus over a specific range of the input sequence.\n",
        "\n",
        "Thus, the first experiments consist in training an RNN model to translate English into French. The model will be composed of an encoder, integrating 1 layer of 256 GRU cells. It feeds a decoder having the same architecture. Between the two modules, there is an attention layer allows the decoder network to “focus” on a different part of the encoder’s outputs for every step of the decoder’s own outputs.\n",
        "\n",
        "![RNN_fig2](https://partage.liris.cnrs.fr/index.php/s/abd8WjgDcdg9T36/preview)\n",
        "\n",
        "\n",
        "<center> Figure 1 - A) Model overview. B) Detailed architecture. </center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyykmrlXbny6"
      },
      "source": [
        "### 4.2 The encoder\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word.\n",
        "\n",
        "![RNN_encoder](https://pytorch.org/tutorials/_images/encoder-network.png) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRC0ETN57HCW"
      },
      "source": [
        "####4.2.1 GRU cells\n",
        "\n",
        "**TODO** Explain what a GRU cell is using the figure below [Fig.2].\n",
        "\n",
        "![GRU](https://partage.liris.cnrs.fr/index.php/s/DDHaH7EQ3yDAeET/preview)\n",
        "\n",
        "<center>Figure 2 - Gated Recurrent Units architecture</center>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORCaGgpoGaxK"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXTa0zTBb0GU"
      },
      "source": [
        "### 4.3 A decoder with attention \n",
        "\n",
        "As the encoder, the decoder is composed of a single layer of GRU cells.\n",
        "\n",
        "Read carefully the paper [1], it is key for the next model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7XZfVNDoZ6m"
      },
      "source": [
        "If only the context vector is passed between the encoder and decoder, that single vector carries the burden of encoding the entire sentence.\n",
        "\n",
        "Attention allows the decoder network to “focus” on a different part of the encoder’s outputs for every step of the decoder’s own outputs. First we calculate a set of attention weights. These will be multiplied by the encoder output vectors to create a weighted combination. The result (called attn_applied in the code) should contain information about that specific part of the input sequence, and thus help the decoder choose the right output words.\n",
        "\n",
        "![attention](https://i.imgur.com/1152PYf.png) \n",
        "\n",
        "Calculating the attention weights is done with another feed-forward layer attn, using the decoder’s input and hidden state as inputs. Because there are sentences of all sizes in the training data, to actually create and train this layer we have to choose a maximum sentence length (input length, for encoder outputs) that it can apply to. Sentences of the maximum length will use all the attention weights, while shorter sentences will only use the first few.\n",
        "\n",
        "![attention2](https://pytorch.org/tutorials/_images/attention-decoder-network.png) \n",
        "\n",
        "<center>Figure 3 - Decoder with attention</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMJAg762oaDA"
      },
      "source": [
        "**TODO** Considering the encoder code, define this layer in the definition of the `AttnDecoderRNN` class, and call it correctly in the forward method of the same class.\n",
        "\n",
        "**TODO** According to the previous diagram (Figure 3), complete the line 25 (`attn_applied`) of the class `AttnDecoderRNN`.\n",
        "\n",
        "**TODO** According to the equation (1) of the paper [1], what plays the role of querry, key and value ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQzCqGFTbzP6"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        # TODO \n",
        "        self.gru =\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        # TODO\n",
        "        # hint: .unsqueeze(0)\n",
        "        attn_applied = \n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        # TODO\n",
        "        output, hidden = \n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HzvBsgDq-ug"
      },
      "source": [
        "### 4.4 Training and evaluation functions\n",
        "\n",
        "It is time to load the set of functions in memory that will allow the RNN to train. \n",
        "\n",
        "#### 4.4.1 Preparing training data\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the words in the input sentence) and target tensor (indexes of the words in the target sentence). While creating these vectors we will append the EOS token to both sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CY5AujeWq1JS"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUIvM4TxtzT9"
      },
      "source": [
        "#### 4.4.2 Training functions\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track of every output and the latest hidden state. Then the decoder is given the <SOS> token as its first input, and the last hidden state of the encoder as its first hidden state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNkATLDMtxj-"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Oul_FDduD74"
      },
      "source": [
        "**TODO** Define what the teacher forcing option is, and give the pros and cons of such an option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCqvLzzTtyrx"
      },
      "source": [
        "#### 4.4.3 Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9M0A_nKbt9ry"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPcAFmV38BGL"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "+ Start a timer\n",
        "\n",
        "+ Initialize optimizers and criterion\n",
        "\n",
        "+ Create set of training pairs\n",
        "\n",
        "+ Start empty losses array for plotting\n",
        "\n",
        "Then we call train many times and occasionally print the progress (% of examples, time so far, estimated time) and average loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6teY9R4t-FA"
      },
      "outputs": [],
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs_train))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)\n",
        "    return plot_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez6_mdUH84e3"
      },
      "source": [
        "#### 4.4.4 Plotting results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltILAI128iH9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uiq6y4LuWcC"
      },
      "source": [
        "#### 4.4.5 Evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW6OP61E86ED"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            \n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dH74EYB89Zg"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs_test)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHwqi6975xVT"
      },
      "outputs": [],
      "source": [
        "def evaluate_all_blue_scores(encoder, decoder):\n",
        "    score = 0\n",
        "    for i in range(len(pairs_test)):\n",
        "        pair = random.choice(pairs_test)\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words[:-1])\n",
        "        score += sentence_bleu([output_sentence.split(' ')], pair[1].split(' '), [(1./3., 1./3., 1./3.) ])\n",
        "    print(\"The BLUE score on the test set is \", score/len(pairs_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYhpBAgG_xef"
      },
      "source": [
        "**TODO** Explain what is the principle of Blue score and what is its limits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCfZ3lSR9pUo"
      },
      "source": [
        "### 4.5 Training and Evaluating\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXEmqoRNMl1S"
      },
      "source": [
        "#### 4.5.1 Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcHz_GkQcazT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL6Tz8--L2KC"
      },
      "outputs": [],
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "plot_losses = trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNUg72xdNGMz"
      },
      "source": [
        "#### 4.5.2 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJraTKn6Lyp2"
      },
      "outputs": [],
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZpxPmTaL4Rj"
      },
      "outputs": [],
      "source": [
        "evaluate_all_blue_scores(encoder1, attn_decoder1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zaszIvsQj_G"
      },
      "outputs": [],
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erxgQ9gUIg_w"
      },
      "source": [
        "## 5.  Transformer model for Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lC0kKuNdsOu"
      },
      "source": [
        "### 5.1 Pre-processing\n",
        "\n",
        "A little more pre-processing must be done to adapt the data to a Transformer model. For this second model, the main difference lies in the use of torchtext.vocab objects to tokenize a raw text sentence and numericalize tokens into tensor. \n",
        "\n",
        "+ Split the data set into 3 sest, training, testing and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcyarzhHy1dD",
        "outputId": "83749286-b2cf-4e7a-95e4-5f538cffa088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10450 1295 626\n"
          ]
        }
      ],
      "source": [
        "fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\n",
        "en_tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "range_pairs = range(len(pairs))\n",
        "prop_train = 0.85\n",
        "prop_val = 0.05\n",
        "pairs_train = []\n",
        "pairs_test = []\n",
        "pairs_val = []\n",
        "for i in range(len(pairs)):\n",
        "    c_rand = random.random()\n",
        "    if c_rand > prop_train + prop_val :\n",
        "        pairs_test.append(pairs[i])\n",
        "    elif c_rand > prop_train :\n",
        "        pairs_val.append(pairs[i])\n",
        "    else:\n",
        "        pairs_train.append(pairs[i])  \n",
        "\n",
        "print(len(pairs_train), len(pairs_test), len(pairs_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvp0XFu6fEqT"
      },
      "source": [
        "+ torchtext has utilities for creating datasets that can be easily iterated through for the purposes of creating a language translation model. In this example, we show how to tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGrQAO2moO7n"
      },
      "outputs": [],
      "source": [
        "    \n",
        "def build_vocab(sets, tokenizer, lang):\n",
        "    counter = Counter()\n",
        "    if lang == 'fr':\n",
        "        p_ind = 0\n",
        "    else:\n",
        "        p_ind = 1\n",
        "    for split in sets:\n",
        "        c_set = split\n",
        "        for i in range(len(c_set)):\n",
        "            c_paris = c_set[i][p_ind]\n",
        "            for word in c_paris.split(' '):\n",
        "                counter.update(tokenizer(word))\n",
        "    return vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "fr_vocab = build_vocab([pairs_train, pairs_test, pairs_val ], fr_tokenizer, 'fr')\n",
        "en_vocab = build_vocab([pairs_train, pairs_test, pairs_val ], en_tokenizer, 'en')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgZ2g82zpl8U"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "  fr_batch, en_batch = [], []\n",
        "  for (fr_item, en_item) in data_batch:\n",
        "    fr_batch.append(torch.cat([torch.tensor([BOS_IDX]), fr_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "  fr_batch = pad_sequence(fr_batch, padding_value=PAD_IDX)\n",
        "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "  return fr_batch, en_batch \n",
        "\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                       shuffle=True, collate_fn=generate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UBlp5Aesapu"
      },
      "source": [
        "+ DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnzrAw68sV29"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "  fr_batch, en_batch = [], []\n",
        "  for (fr_item, en_item) in data_batch:\n",
        "    fr_batch.append(torch.cat([torch.tensor([BOS_IDX]), fr_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "  fr_batch = pad_sequence(fr_batch, padding_value=PAD_IDX)\n",
        "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "  return en_batch , fr_batch\n",
        "\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                       shuffle=True, collate_fn=generate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z4v5L24q10x"
      },
      "source": [
        "### 5.2 Transformer overview\n",
        "\n",
        "Transformer is a Seq2Seq model introduced in “Attention is all you need” paper for solving machine translation task. Transformer model consists of an encoder and decoder block each containing fixed number of layers.\n",
        "\n",
        "Encoder processes the input sequence by propogating it, through a series of Multi-head Attention and Feed forward network layers. The output from the Encoder referred to as memory, is fed to the decoder along with target tensors. Encoder and decoder are trained in an end-to-end fashion using teacher forcing technique.\n",
        "\n",
        "The core of Transformers is self-attention mechanisms. The main advantages of Transformer over compared to an RNN model are [2] :\n",
        "\n",
        "— Parallel processing of all elements of the sequence, since it is not based on recurrent computation.\n",
        "\n",
        "— It estimates the relevance of an element in relation to all other elements, by modeling the interactions of all entities in the sequence.\n",
        "\n",
        "— Thus it takes into account the global information of a sequence.\n",
        "\n",
        "![Transformers](https://partage.liris.cnrs.fr/index.php/s/FEadbZr6wSQEte9/preview)\n",
        "\n",
        "<center>Figure 4 - Structure of Transformer (from [1])</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv5sEYW3yh-N"
      },
      "source": [
        "### 5.3 Self-Attention\n",
        "\n",
        "**TODO** Describe the self-attention according to the paper \"Attention is all you need\" [1].\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iB4pOV3ypQb"
      },
      "source": [
        "### 5.4 Multi-Head Attention\n",
        "\n",
        "In order to encapsulate multiple relationships between the elements, *N* encoder and decoder modules are put in parallel [Fig.4, 5]. Defining *h* self-head attention blocks, each block owns a set of learnable weight matrix *{W<sup>Q,i</sup>,W<sup>K,i</sup>,W<sup>V,i</sup>}* where *i = 0, ...(h − 1)*. Like this, for an input *X* the outputs of the *h* self-attention blocks are concatenated into a single matrix *\\[Z<sub>0</sub>, Z<sub>1</sub>, .., Z<sub>h−1</sub>\\]* ∈ *R<sup>n×h.d<sub>v</sub></sup>* and projected on a weight matrix *W* ∈ *R<sup>h.d<sub>v</sub>×d</sup>*.\n",
        "\n",
        "![MultiHeadAttention](https://partage.liris.cnrs.fr/index.php/s/s4GAM9P2NcyJw4B/preview)\n",
        "\n",
        "<center>Figure 5 - Multi-Head Attention [1].</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT2DMYTczvtp"
      },
      "source": [
        "### 5.5 Other key elements fo Transformer\n",
        "\n",
        "#### 5.5.1 Positional embedding\n",
        "\n",
        "To not consider the sequences like bags of words, words embedding are concatenated with their positional embedding vector [Fig.4]. Considering that each word is embedded into a vector of dimension *d<sub>m</sub>*, each element of the positional vector *POS* ∈ *R<sup>d</sup>* results from the following equation :\n",
        "\n",
        "![equation](https://partage.liris.cnrs.fr/index.php/s/TpZPMCp7fERm22X/preview)\n",
        "\n",
        "where *pos* is the integer corresponding to the word position in the sequence and *i* is the current entry in *POS*, the even and odd entry result respectively from (eq.1) and (eq.2).\n",
        "\n",
        "#### 5.5.2 Residual connection and normalization layer\n",
        "\n",
        "Residual connections and normalization layers are added to each sub-layer of the encoder-decoder module [Fig.4] such that :\n",
        "\n",
        "![equation_layer_norm](https://partage.liris.cnrs.fr/index.php/s/7QJQigD7K8B5Ki6/preview)\n",
        "\n",
        "\n",
        "#### 5.5.3 Feed-forward module\n",
        "\n",
        "Multi-headed attention modules are tracked by feed-forward networks, consisting of two linear layers that are followed by a non-linear activation function [Fig.4] [3].\n",
        "\n",
        "#### 5.5.4 Final layer of the decoder\n",
        "\n",
        "To turn the stack of vectors back into a word, a linear layer projects the final vector into a logit vector of dimension *d<sub>vocab</sub>*. Then a softmax layer is used to turn the logits into probabilities [3]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0-5J6sQ1RQY"
      },
      "source": [
        "### 5.6 Transformer encoder and decoder\n",
        "\n",
        "**TODO** Complete the initialization of the `Seq2SeqTransformer` class by defining the decoder.\n",
        "\n",
        "Then, load the model by running the code in following cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmAMmG0zsuCm"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,\n",
        "                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,\n",
        "                 dim_feedforward:int = 512, dropout:float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=NHEAD,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        # TODO\n",
        "        decoder_layer = \n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,\n",
        "                tgt_mask: Tensor, src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
        "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n",
        "                                        tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer_encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer_decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoK7dE-Jszs-"
      },
      "source": [
        "Text tokens are represented by using token embeddings. Positional encoding is added to the token embedding to introduce a notion of word order.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRdspcQ5suGN"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding +\n",
        "                            self.pos_embedding[:token_embedding.size(0),:])\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMTZuRn_s7UP"
      },
      "source": [
        "We create a subsequent word mask to stop a target word from attending to its subsequent words. We also create masks, for masking source and target padding tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMp173W5s6lb"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "  src_seq_len = src.shape[0]\n",
        "  tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "  tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "  src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
        "\n",
        "  src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "  tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "  return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCjs70uWtAlW"
      },
      "source": [
        "### 5.7  Training and evaluation functions for Transformer\n",
        "\n",
        "**TODO** Complete the following training configuration by setting the variable `transformer` according to the definition of `Seq2SeqTransformer`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_Mb5HN-suJ8"
      },
      "outputs": [],
      "source": [
        "TGT_VOCAB_SIZE = len(fr_vocab)\n",
        "SRC_VOCAB_SIZE = len(en_vocab)\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "NUM_EPOCHS = 16\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# TODO\n",
        "transformer = \n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERnvSdbFsuM6"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_iter, optimizer):\n",
        "  model.train()\n",
        "  losses = 0\n",
        "  for idx, ( src ,tgt  ) in enumerate(train_iter):\n",
        "      src = src.to(device)\n",
        "      tgt = tgt.to(device)\n",
        "\n",
        "      tgt_input = tgt[:-1, :]\n",
        "\n",
        "      src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "      logits = model(src, tgt_input, src_mask, tgt_mask,\n",
        "                                src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      tgt_out = tgt[1:,:]\n",
        "      loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      losses += loss.item()\n",
        "  return losses / len(train_iter)\n",
        "\n",
        "\n",
        "def evaluate(model, val_iter):\n",
        "  model.eval()\n",
        "  losses = 0\n",
        "  for idx, ( src, tgt ) in (enumerate(valid_iter)):\n",
        "    src = src.to(device)\n",
        "    tgt = tgt.to(device)\n",
        "\n",
        "    tgt_input = tgt[:-1, :]\n",
        "\n",
        "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "    logits = model(src, tgt_input, src_mask, tgt_mask,\n",
        "                              src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "    tgt_out = tgt[1:,:]\n",
        "    loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "    losses += loss.item()\n",
        "  return losses / len(val_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip3_wASYtQCW"
      },
      "source": [
        "\n",
        "**TODO** Edit the following cell to record all the values for epoch, train_loss and val_loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k--KSV_TsuPk",
        "outputId": "51017e16-8a25-4360-a694-38e86019a054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train loss: 5.217, Val loss: 3.850, Epoch time = 305.383s\n",
            "Epoch: 2, Train loss: 3.529, Val loss: 3.202, Epoch time = 302.178s\n"
          ]
        }
      ],
      "source": [
        "# TODO:  save all values of epoch, train_loss and val_loss\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "  start_time = time.time()\n",
        "  train_loss = train_epoch(transformer, train_iter, optimizer)\n",
        "  end_time = time.time()\n",
        "  val_loss = evaluate(transformer, valid_iter)\n",
        "  print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
        "          f\"Epoch time = {(end_time - start_time):.3f}s\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0AFMWF320QR"
      },
      "source": [
        "**TODO** Plot the train and validation losses on the same graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O53UwKitn_ui"
      },
      "outputs": [],
      "source": [
        "#TODO : Plot the train and validation losses on the same graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6pPa8CGxQan"
      },
      "source": [
        "### 5.8 Transformer evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ6pWrBktU0T"
      },
      "outputs": [],
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(device)\n",
        "    src_mask = src_mask.to(device)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(device)\n",
        "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                                    .type(torch.bool)).to(device)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "          break\n",
        "    return ys\n",
        "\n",
        "\n",
        "def translate(model, src, src_vocab, tgt_vocab, src_tokenizer):\n",
        "  model.eval()\n",
        "  src_stoi = src_vocab.get_stoi()\n",
        "  tokens = [BOS_IDX] + [src_stoi[tok] for tok in src_tokenizer(src)]+ [EOS_IDX]\n",
        "  num_tokens = len(tokens)\n",
        "  src = (torch.LongTensor(tokens).reshape(num_tokens, 1) )\n",
        "  src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "  tgt_tokens = greedy_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "  tgt_itos = tgt_vocab.get_itos()\n",
        "\n",
        "  print(\"=\", \" \".join([tgt_itos[tok] for tok in tgt_tokens]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
        "  return \" \".join([tgt_itos[tok] for tok in tgt_tokens]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK9lJ8PPorDG"
      },
      "outputs": [],
      "source": [
        "# TODO: run the function translate on 15 examples from the test. Print the src and the tgt sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znLgOS_dyct8"
      },
      "source": [
        "### Get blue score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU0K_MmCycI1"
      },
      "outputs": [],
      "source": [
        "def evaluate_transformer_all_blue_score(model, pairs, src_vocab, tgt_vocab, src_tokenizer):\n",
        "    # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1k3Kk6Lye52"
      },
      "outputs": [],
      "source": [
        " # TODO: Call evaluate_transformer_all_blue_score on your the set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUH_su624liA"
      },
      "source": [
        "### 5.9 Conclusion\n",
        "\n",
        "**TODO** : Write a conclusion comparing your expericence with a RNN and a Transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEpChSFa4wYs"
      },
      "source": [
        "##6. Références\n",
        "\n",
        "[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. [arXiv preprint arXiv :1706.03762, 2017.](https://arxiv.org/pdf/1706.03762.pdf)\n",
        "\n",
        "[2] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision : A survey. [arXiv preprint arXiv :2101.01169, 2021.](https://arxiv.org/pdf/2101.01169.pdf)\n",
        "\n",
        "[3] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. A survey on visual transformer. [arXiv preprint arXiv :2012.12556, 2020.](https://arxiv.org/pdf/2012.12556.pdf)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
